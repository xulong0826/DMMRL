import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from torch_geometric.nn import global_mean_pool, GlobalAttention
from torch.nn import init
from torch.nn.parameter import Parameter
from models_lib.gnn_model import MPNEncoder
from models_lib.gem_model import GeoGNNModel
from models_lib.seq_model import TrfmSeq2seq

loss_type = {'class': nn.BCEWithLogitsLoss(reduction="none"), 'reg': nn.MSELoss(reduction="none")}

# 🔧 改进的归一化函数
def adaptive_normalize(x, dim=1, eps=1e-6, strength=0.7):
    """自适应归一化，减少过度归一化的负面影响"""
    if x.size(0) < 2:  # 小batch不归一化
        return x
    
    # 检查是否需要归一化
    std = torch.std(x, dim=dim, keepdim=True)
    if (std < eps).any():  # 如果标准差太小，跳过归一化
        return x
    
    # 更温和的归一化
    mean = torch.mean(x, dim=dim, keepdim=True)
    normalized = (x - mean) / (std + eps)
    # 部分保留原始信息
    return strength * normalized + (1 - strength) * x

class Global_Attention(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.at = GlobalAttention(gate_nn=torch.nn.Linear(hidden_size, 1))

    def forward(self, x, batch):
        return self.at(x, batch)

class WeightFusion(nn.Module):
    def __init__(self, feat_views, feat_dim, bias: bool = True, device=None, dtype=None) -> None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super(WeightFusion, self).__init__()
        self.feat_views = feat_views
        self.feat_dim = feat_dim
        self.weight = Parameter(torch.empty((feat_views,), **factory_kwargs))
        if bias:
            self.bias = Parameter(torch.empty(int(feat_dim), **factory_kwargs))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self) -> None:
        # 更保守的初始化
        init.uniform_(self.weight, 0.8, 1.2)  # 接近均匀权重
        if self.bias is not None:
            init.zeros_(self.bias)

    def forward(self, input_list) -> Tensor:
        # 🔧 修正权重和输入处理
        if not isinstance(input_list, list):
            raise ValueError("Input must be a list of tensors")
        if len(input_list) != self.feat_views:
            raise ValueError(f"Expected {self.feat_views} input tensors, got {len(input_list)}")
        
        # 检查所有张量维度是否一致
        expected_dim = input_list[0].size(-1)
        if expected_dim != self.feat_dim:
            print(f"Warning: Expected dim {self.feat_dim}, got {expected_dim}")
        
        # 使用softmax确保权重和为1
        weights = F.softmax(self.weight, dim=0)
        
        # 加权求和
        result = torch.zeros_like(input_list[0])
        for i, tensor in enumerate(input_list):
            if tensor.size(-1) != result.size(-1):
                raise RuntimeError(f"Tensor {i} has incompatible size: {tensor.shape} vs expected last dim {result.size(-1)}")
            result += tensor * weights[i]
        
        return result + self.bias if self.bias is not None else result

class VAEHead(nn.Module):
    def __init__(self, in_dim, shared_dim, private_dim, hidden_dim=256, dropout=0.3, norm=False):
        super().__init__()
        self.norm_enabled = norm
        
        # 🔧 简化编码器结构，减少归一化层
        self.encoder_shared = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout * 0.5)
        )
        
        self.encoder_private = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout * 0.5)
        )
        
        # 变分参数层
        hidden_final = hidden_dim // 2
        self.fc_mu_shared = nn.Linear(hidden_final, shared_dim)
        self.fc_logvar_shared = nn.Linear(hidden_final, shared_dim)
        self.fc_mu_private = nn.Linear(hidden_final, private_dim)
        self.fc_logvar_private = nn.Linear(hidden_final, private_dim)

        # 🔧 简化解码器
        self.decoder = nn.Sequential(
            nn.Linear(shared_dim + private_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout * 0.3),
            nn.Linear(hidden_dim, in_dim)
        )
        
        # 可选的归一化层
        if norm:
            self.input_norm = nn.LayerNorm(in_dim, eps=1e-6)
        else:
            self.input_norm = nn.Identity()
        
        self._init_weights()

    def _init_weights(self):
        """保守的权重初始化"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.02)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
        
        # 🔧 保守的logvar初始化
        with torch.no_grad():
            self.fc_logvar_shared.weight.fill_(0.0)
            self.fc_logvar_shared.bias.fill_(-4.0)
            self.fc_logvar_private.weight.fill_(0.0)
            self.fc_logvar_private.bias.fill_(-4.0)

    def reparameterize(self, mu, logvar):
        if self.training:
            # 🔧 更加保守的重参数化
            logvar = torch.clamp(logvar, min=-8, max=1)
            std = torch.exp(0.5 * logvar) * 0.5
            eps = torch.randn_like(std) * 0.3
            return mu + eps * std
        else:
            return mu

    def forward(self, x):
        # 🔧 只在必要时归一化一次
        if self.norm_enabled:
            x = self.input_norm(x)
        
        h_shared = self.encoder_shared(x)
        h_private = self.encoder_private(x)
        
        mu_shared = self.fc_mu_shared(h_shared)
        logvar_shared = torch.clamp(self.fc_logvar_shared(h_shared), min=-8, max=1)
        mu_private = self.fc_mu_private(h_private)
        logvar_private = torch.clamp(self.fc_logvar_private(h_private), min=-8, max=1)
            
        z_shared = self.reparameterize(mu_shared, logvar_shared)
        z_private = self.reparameterize(mu_private, logvar_private)
        
        z = torch.cat([z_shared, z_private], dim=1)
        recon_x = self.decoder(z)
        
        return z_shared, z_private, recon_x, mu_shared, logvar_shared, mu_private, logvar_private

class Multi_modal(nn.Module):
    def __init__(self, args, compound_encoder_config, device):
        super().__init__()
        self.args = args
        self.device = device
        self.latent_dim = args.latent_dim
        self.batch_size = args.batch_size
        self.graph = args.graph
        self.sequence = args.sequence
        self.geometry = args.geometry
        self.shared_dim = args.latent_dim // 4
        self.private_dim = args.latent_dim // 4 * 3
        
        # 🔧 预计算常量
        self.use_norm = bool(args.norm)
        self.num_modalities = self.graph + self.sequence + self.geometry

        self.gnn = MPNEncoder(atom_fdim=args.gnn_atom_dim, bond_fdim=args.gnn_bond_dim,
                              hidden_size=args.gnn_hidden_dim, bias=args.bias, depth=args.gnn_num_layers,
                              dropout=args.dropout, activation=args.gnn_activation, device=device)
        self.transformer = TrfmSeq2seq(input_dim=args.seq_input_dim, hidden_size=args.seq_hidden_dim,
                                       num_head=args.seq_num_heads, n_layers=args.seq_num_layers, dropout=args.dropout,
                                       vocab_num=args.vocab_num, device=device, recons=args.recons).to(self.device)
        self.compound_encoder = GeoGNNModel(args, compound_encoder_config, device)

        # 🔧 更保守的VAE设置
        vae_dropout = max(0.1, args.dropout * 0.6)
        self.gnn_ae = VAEHead(args.gnn_hidden_dim, self.shared_dim, self.private_dim, 
                              hidden_dim=max(128, args.gnn_hidden_dim // 2),
                              dropout=vae_dropout, norm=self.use_norm).to(device)
        self.seq_ae = VAEHead(args.seq_hidden_dim, self.shared_dim, self.private_dim, 
                              hidden_dim=max(128, args.seq_hidden_dim // 2),
                              dropout=vae_dropout, norm=self.use_norm).to(device)
        self.geo_ae = VAEHead(args.geo_hidden_dim, self.shared_dim, self.private_dim, 
                              hidden_dim=max(128, args.geo_hidden_dim // 2),
                              dropout=vae_dropout, norm=self.use_norm).to(device)

        # 🔧 正交投影层
        self.private2shared_proj = nn.Sequential(
            nn.Linear(self.private_dim, self.shared_dim),
            nn.GELU(),
            nn.Linear(self.shared_dim, self.shared_dim)
        ).to(device)

        # 投影层（如果需要）
        if hasattr(args, 'pro_num') and args.pro_num == 3:
            proj_dim = max(128, self.latent_dim // 2)
            self.pro_seq = nn.Sequential(
                nn.Linear(args.seq_hidden_dim, proj_dim), 
                nn.GELU(),
                nn.Dropout(args.dropout * 0.5),
                nn.Linear(proj_dim, self.latent_dim)
            ).to(device)
            self.pro_gnn = nn.Sequential(
                nn.Linear(args.gnn_hidden_dim, proj_dim), 
                nn.GELU(),
                nn.Dropout(args.dropout * 0.5),
                nn.Linear(proj_dim, self.latent_dim)
            ).to(device)
            self.pro_geo = nn.Sequential(
                nn.Linear(args.geo_hidden_dim, proj_dim), 
                nn.GELU(),
                nn.Dropout(args.dropout * 0.5),
                nn.Linear(proj_dim, self.latent_dim)
            ).to(device)

        self.entropy = loss_type[args.task_type]

        if args.pool_type == 'mean':
            self.pool = global_mean_pool
        else:
            self.pool = Global_Attention(args.seq_hidden_dim).to(self.device)

        # 🔧 修正融合维度计算
        single_modality_dim = self.shared_dim + args.gnn_hidden_dim  # 假设所有模态hidden_dim相同
        
        if self.args.fusion == 3:
            # WeightFusion: 输入是多个相同维度的张量
            self.fusion = WeightFusion(self.num_modalities, single_modality_dim, device=self.device)
            fusion_dim = single_modality_dim
        elif self.args.fusion == 2 or self.args.fusion == 0:
            fusion_dim = args.seq_hidden_dim
        else:
            # 直接拼接所有模态
            fusion_dim = single_modality_dim * self.num_modalities

        # 🔧 更保守的输出层
        self.dropout = nn.Dropout(args.dropout * 0.7)
        
        output_hidden = max(64, int(fusion_dim) // 2)
        self.output_layer = nn.Sequential(
            nn.Linear(int(fusion_dim), output_hidden),
            nn.GELU(),
            nn.Dropout(args.dropout * 0.3),
            nn.Linear(output_hidden, args.output_dim)
        ).to(self.device)

    def forward(self, trans_batch_seq, seq_mask, batch_mask_seq, gnn_batch_graph, 
                gnn_feature_batch, batch_mask_gnn, graph_dict, node_id_all, edge_id_all):
        
        shared_list, private_list = [], []
        mu_shared_list, logvar_shared_list = [], []
        mu_private_list, logvar_private_list = [], []
        recon_list, orig_list = [], []

        if self.graph:
            node_gnn_x = self.gnn(gnn_batch_graph, gnn_feature_batch, batch_mask_gnn)
            graph_gnn_x = self.pool(node_gnn_x, batch_mask_gnn)
            
            z_shared, z_private, recon, mu_s, logvar_s, mu_p, logvar_p = self.gnn_ae(graph_gnn_x)
            shared_list.append(z_shared)
            private_list.append(z_private)
            mu_shared_list.append(mu_s)
            logvar_shared_list.append(logvar_s)
            mu_private_list.append(mu_p)
            logvar_private_list.append(logvar_p)
            recon_list.append(recon)
            
            # 🔧 条件归一化
            if self.use_norm and graph_gnn_x.size(0) > 1:
                orig_list.append(adaptive_normalize(graph_gnn_x, dim=1, strength=0.5))
            else:
                orig_list.append(graph_gnn_x)

        if self.sequence:
            nloss, node_seq_x = self.transformer(trans_batch_seq)
            graph_seq_x = self.pool(node_seq_x[seq_mask], batch_mask_seq)
            
            z_shared, z_private, recon, mu_s, logvar_s, mu_p, logvar_p = self.seq_ae(graph_seq_x)
            shared_list.append(z_shared)
            private_list.append(z_private)
            mu_shared_list.append(mu_s)
            logvar_shared_list.append(logvar_s)
            mu_private_list.append(mu_p)
            logvar_private_list.append(logvar_p)
            recon_list.append(recon)

            if self.use_norm and graph_seq_x.size(0) > 1:
                orig_list.append(adaptive_normalize(graph_seq_x, dim=1, strength=0.5))
            else:
                orig_list.append(graph_seq_x)

        if self.geometry:
            node_repr, edge_repr = self.compound_encoder(graph_dict[0], graph_dict[1], node_id_all, edge_id_all)
            graph_geo_x = self.pool(node_repr, node_id_all[0])
            
            z_shared, z_private, recon, mu_s, logvar_s, mu_p, logvar_p = self.geo_ae(graph_geo_x)
            shared_list.append(z_shared)
            private_list.append(z_private)
            mu_shared_list.append(mu_s)
            logvar_shared_list.append(logvar_s)
            mu_private_list.append(mu_p)
            logvar_private_list.append(logvar_p)
            recon_list.append(recon)

            if self.use_norm and graph_geo_x.size(0) > 1:
                orig_list.append(adaptive_normalize(graph_geo_x, dim=1, strength=0.5))
            else:
                orig_list.append(graph_geo_x)

        # 🔧 修正特征融合
        if self.args.fusion == 3:
            # WeightFusion需要相同维度的张量列表
            residual_list = []
            for s, o in zip(shared_list, orig_list):
                combined = torch.cat([s, o], dim=1)
                residual_list.append(combined)
            molecule_emb = self.fusion(residual_list)
        else:
            # 其他融合方式：直接拼接
            residual_list = []
            for s, o in zip(shared_list, orig_list):
                if self.use_norm:
                    combined = torch.cat([s * 0.7, o * 0.3], dim=1)
                else:
                    combined = torch.cat([s, o], dim=1)
                residual_list.append(combined)
            molecule_emb = torch.cat(residual_list, dim=1)
        
        # 🔧 条件dropout
        if not self.use_norm:
            molecule_emb = self.dropout(molecule_emb)
        
        preds = self.output_layer(molecule_emb)
        
        # 🔧 数值稳定性检查
        if torch.isnan(preds).any() or torch.isinf(preds).any():
            print("Warning: NaN/Inf in predictions, using backup prediction")
            preds = torch.zeros_like(preds)
            
        return shared_list, private_list, preds, recon_list, orig_list, \
               mu_shared_list, logvar_shared_list, mu_private_list, logvar_private_list

    def label_loss(self, pred, label, mask):
        loss_mat = self.entropy(pred, label)
        return loss_mat.sum() / (mask.sum() + 1e-8)

    def ae_loss(self, recon_list, orig_list):
        if not recon_list:
            return torch.tensor(0.0, device=self.device)
        
        # 🔧 Huber损失替代MSE
        loss = 0
        for recon, orig in zip(recon_list, orig_list):
            loss += F.smooth_l1_loss(recon, orig, reduction='mean')
        return loss / len(recon_list)

    def kl_loss(self, mu_list, logvar_list):
        if not mu_list:
            return torch.tensor(0.0, device=self.device)
            
        kl = 0
        for mu, logvar in zip(mu_list, logvar_list):
            # 🔧 更稳定的KL计算
            kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
            kl_div = torch.clamp(kl_div, min=0.0, max=5.0)
            kl += kl_div
        return kl / len(mu_list)

    def ortho_loss(self, z_shared_list, z_private_list):
        if not z_shared_list or not z_private_list:
            return torch.tensor(0.0, device=self.device)
            
        loss = 0
        for z_s, z_p in zip(z_shared_list, z_private_list):
            # 🔧 温和的正交损失
            z_s_norm = adaptive_normalize(z_s, dim=1, strength=0.3)
            z_p_proj = self.private2shared_proj(z_p)
            z_p_proj_norm = adaptive_normalize(z_p_proj, dim=1, strength=0.3)
            
            # 使用余弦相似度
            cos_sim = F.cosine_similarity(z_s_norm, z_p_proj_norm, dim=1)
            loss += torch.mean(torch.abs(cos_sim))
        return loss / len(z_shared_list)

    def align_loss(self, z_shared_list):
        if len(z_shared_list) < 2:
            return torch.tensor(0.0, device=self.device)
            
        loss = 0
        count = 0
        for i in range(len(z_shared_list)):
            for j in range(i+1, len(z_shared_list)):
                # 🔧 更稳定的对齐损失
                z_i_norm = adaptive_normalize(z_shared_list[i], dim=1, strength=0.3)
                z_j_norm = adaptive_normalize(z_shared_list[j], dim=1, strength=0.3)
                cos_sim = F.cosine_similarity(z_i_norm, z_j_norm, dim=1)
                loss += torch.mean(1 - cos_sim)
                count += 1
        return loss / count if count > 0 else torch.tensor(0.0, device=self.device)

    def loss_cal(self, epoch, preds, targets, mask, recon_list, orig_list, 
                 mu_shared_list, logvar_shared_list, mu_private_list, logvar_private_list,
                 z_shared_list, z_private_list,
                 recon_weight=0.05, beta_shared=0.005, beta_private=0.005, 
                 gamma_ortho=0.001, gamma_align=0.001, kl_warmup_epochs=40):
        
        # 🔧 更温和的KL权重调度
        kl_weight = 0.3 * (1 - math.cos(math.pi * min(1.0, epoch / kl_warmup_epochs)))
        
        # 计算各损失项
        loss_label = self.label_loss(preds, targets, mask)
        recon_loss = self.ae_loss(recon_list, orig_list)
        kl_shared = self.kl_loss(mu_shared_list, logvar_shared_list)
        kl_private = self.kl_loss(mu_private_list, logvar_private_list)
        ortho = self.ortho_loss(z_shared_list, z_private_list)
        align = self.align_loss(z_shared_list)
        
        # 🔧 自适应权重调整
        label_scale = min(1.0, 1.0 / (loss_label.item() + 1e-8))
        reg_scale = max(0.1, label_scale)
        
        total_loss = (
            loss_label +
            recon_weight * reg_scale * recon_loss +
            kl_weight * reg_scale * (beta_shared * kl_shared + beta_private * kl_private) +
            kl_weight * reg_scale * (gamma_ortho * ortho + gamma_align * align)
        )
        
        # 🔧 数值稳定性检查
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print("Warning: NaN/Inf in total loss, using label loss only")
            total_loss = loss_label
            
        return total_loss, loss_label, recon_loss